<h1>Overview of neural network architecture</h1>
<h2>Intuitive understanding</h2>
<p>A neural network is pretty much just a function that maps a bunch of inputs to a bunch of outputs. First that function does bad at mapping. By showing a lot of input/output pairs the parameters in the function get adjusted to improve the mapping.</p>
<p>So there are three big parts of a neural network. The architecture of the network, the optimization of the parameters and the amount and quality of the data.</p>
<h2>Architecture</h2>
<ul>
<li>How many layers?</li>
<li>What type of layers?</li>
<li>What activation functions?</li>
<li>Input and output dimensions?</li>
</ul>
<h2>Optimization</h2>
<ul>
<li>What does the loss function look like?</li>
<li>Gradient descent?</li>
<li>What optimizer?</li>
<li>When and how fast to change the parameters?</li>
<li>When to stop training?</li>
<li>Is there overfitting?</li>
</ul>
<h2>Data</h2>
<ul>
<li>How much data is there?</li>
<li>Is Data argumentation necessary and/or useful?</li>
<li>Can there be too much data?</li>
<li>Is there bias in data?</li>
</ul>
<h1>Practical Stuff</h1>
<h2>Perceptron</h2>
<p>The Perceptron is the simplest neural network possible.</p>
<h2>Implement small deep learning library from scratch (with numpy)</h2>
<p>At some point!! To help with a deeper understanding of backpropagation and the inner workings in general.</p>
